{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73d800c",
   "metadata": {},
   "source": [
    "# Build a Model\n",
    "\n",
    "Dan thinks a next great step: \n",
    "* basis of building a model.\n",
    "* i.e. try to predict star rating (1-5) based off review text... in class different options for taking in review text and converting it to something we can reason about.\n",
    "* For each review, create something that is a 1-hot encoding of that review. i.e. for all words used in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # some possible useful functions courtesy of DL exercise chat: \n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# from sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn import metrics\n",
    "\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(reviews, ratings)\n",
    "\n",
    "# lr = LogisticRegression(C=100.0, random_state=1, solver='lbfgs', multi_class='ovr')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886ac042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332ee026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e593e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf19dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in grocery and gourmet food data\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fee1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getDF('Grocery_and_Gourmet_Food_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47806980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>11 19, 2014</td>\n",
       "      <td>A1QVBUH9E1V6I8</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>Jamshed Mathur</td>\n",
       "      <td>No adverse comment.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1416355200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>10 13, 2016</td>\n",
       "      <td>A3GEOILWLK86XM</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>itsjustme</td>\n",
       "      <td>Gift for college student.</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>1476316800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>11 21, 2015</td>\n",
       "      <td>A32RD6L701BIGP</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>Krystal Clifton</td>\n",
       "      <td>If you like strong tea, this is for you. It mi...</td>\n",
       "      <td>Strong</td>\n",
       "      <td>1448064000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>08 12, 2015</td>\n",
       "      <td>A2UY1O1FBGKIE6</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>U. Kane</td>\n",
       "      <td>Love the tea. The flavor is way better than th...</td>\n",
       "      <td>Great tea</td>\n",
       "      <td>1439337600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>05 28, 2015</td>\n",
       "      <td>A3QHVBQYDV7Z6U</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>The Nana</td>\n",
       "      <td>I have searched everywhere until I browsed Ama...</td>\n",
       "      <td>This is the tea I remembered!</td>\n",
       "      <td>1432771200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0      5.0      True  11 19, 2014  A1QVBUH9E1V6I8  4639725183   \n",
       "1      5.0      True  10 13, 2016  A3GEOILWLK86XM  4639725183   \n",
       "2      5.0      True  11 21, 2015  A32RD6L701BIGP  4639725183   \n",
       "3      5.0      True  08 12, 2015  A2UY1O1FBGKIE6  4639725183   \n",
       "4      5.0      True  05 28, 2015  A3QHVBQYDV7Z6U  4639725183   \n",
       "\n",
       "      reviewerName                                         reviewText  \\\n",
       "0   Jamshed Mathur                                No adverse comment.   \n",
       "1        itsjustme                          Gift for college student.   \n",
       "2  Krystal Clifton  If you like strong tea, this is for you. It mi...   \n",
       "3          U. Kane  Love the tea. The flavor is way better than th...   \n",
       "4         The Nana  I have searched everywhere until I browsed Ama...   \n",
       "\n",
       "                         summary  unixReviewTime vote style image  \n",
       "0                     Five Stars      1416355200  NaN   NaN   NaN  \n",
       "1                 Great product.      1476316800  NaN   NaN   NaN  \n",
       "2                         Strong      1448064000  NaN   NaN   NaN  \n",
       "3                      Great tea      1439337600  NaN   NaN   NaN  \n",
       "4  This is the tea I remembered!      1432771200  NaN   NaN   NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea91b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interested in columns overall and reviewText\n",
    "\n",
    "df_preproc = cleaner(df)\n",
    "df_preproc.head(3) = df[[\"overall\", \"reviewText\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9994eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count is: 1143860\n",
      "Number of reviewText null is:  390\n",
      "Number of overall null is:  0\n",
      "Clean row count is: 1143470\n"
     ]
    }
   ],
   "source": [
    "# Clean reviewText\n",
    "\n",
    "# import spacy \n",
    "print(\"Row count is:\", len(df_all.index))\n",
    "print(\"Number of reviewText null is: \", df_all[\"reviewText\"].isnull().values.sum())\n",
    "print(\"Number of overall null is: \", df_all[\"overall\"].isnull().values.sum())\n",
    "\n",
    "df_all = df_all.dropna()\n",
    "print(\"Clean row count is:\", len(df_all.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1585ee33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ad83538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use subset for now - write code to work with just 1000?\n",
    "n_data_samples = 1000\n",
    "df_data = df_all.head(n_data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0a2852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>No adverse comment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Gift for college student.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>If you like strong tea, this is for you. It mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Love the tea. The flavor is way better than th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I have searched everywhere until I browsed Ama...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText\n",
       "0      5.0                                No adverse comment.\n",
       "1      5.0                          Gift for college student.\n",
       "2      5.0  If you like strong tea, this is for you. It mi...\n",
       "3      5.0  Love the tea. The flavor is way better than th...\n",
       "4      5.0  I have searched everywhere until I browsed Ama..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f9e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e61913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Use spacy to remove stop words and craete list of words for each reviewText row?\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# spacy pipeline capability. Rather than mapping directly. \n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "stop = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "# Should all these transforms be in my read function? Probably... so I can load data efficiently. \n",
    "\n",
    "# split string into list, all lower case, remove punctuation, remove stop words\n",
    "df_data['text_split'] = df_data['reviewText'].str.replace('[^\\w\\s]','').str.lower().str.split()\n",
    "df_data['text_no_stop'] = df_data['text_split'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe4e0853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_no_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>No adverse comment.</td>\n",
       "      <td>[no, adverse, comment]</td>\n",
       "      <td>[adverse, comment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Gift for college student.</td>\n",
       "      <td>[gift, for, college, student]</td>\n",
       "      <td>[gift, college, student]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>If you like strong tea, this is for you. It mi...</td>\n",
       "      <td>[if, you, like, strong, tea, this, is, for, yo...</td>\n",
       "      <td>[like, strong, tea, little, strong]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Love the tea. The flavor is way better than th...</td>\n",
       "      <td>[love, the, tea, the, flavor, is, way, better,...</td>\n",
       "      <td>[love, tea, flavor, way, better, regular, lipt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I have searched everywhere until I browsed Ama...</td>\n",
       "      <td>[i, have, searched, everywhere, until, i, brow...</td>\n",
       "      <td>[searched, browsed, amazon, tea, lipton, selli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText  \\\n",
       "0      5.0                                No adverse comment.   \n",
       "1      5.0                          Gift for college student.   \n",
       "2      5.0  If you like strong tea, this is for you. It mi...   \n",
       "3      5.0  Love the tea. The flavor is way better than th...   \n",
       "4      5.0  I have searched everywhere until I browsed Ama...   \n",
       "\n",
       "                                          text_split  \\\n",
       "0                             [no, adverse, comment]   \n",
       "1                      [gift, for, college, student]   \n",
       "2  [if, you, like, strong, tea, this, is, for, yo...   \n",
       "3  [love, the, tea, the, flavor, is, way, better,...   \n",
       "4  [i, have, searched, everywhere, until, i, brow...   \n",
       "\n",
       "                                        text_no_stop  \n",
       "0                                 [adverse, comment]  \n",
       "1                           [gift, college, student]  \n",
       "2                [like, strong, tea, little, strong]  \n",
       "3  [love, tea, flavor, way, better, regular, lipt...  \n",
       "4  [searched, browsed, amazon, tea, lipton, selli...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e59d9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # load an existing English template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99e6baa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30264/699128994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_lammaterized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_split'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df_data['text_lammaterized'] = [k_word.lemma_ for i_list in df_data['text_split'] for j_word in i_list for k_word in nlp(j_word)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30264/699128994.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_lammaterized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_split'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df_data['text_lammaterized'] = [k_word.lemma_ for i_list in df_data['text_split'] for j_word in i_list for k_word in nlp(j_word)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE1041\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     def _ensure_doc_with_context(\n",
      "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99741d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cafa115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv(\"df_data_clean.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c09382c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa40c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7fbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73ce821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do better\n",
    "# FOlllowing https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1989b4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7f70f665d690>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c408417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "stopwords = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86edc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleaner(df):\n",
    "    \"Extract relevant text from DataFrame using a regex\"\n",
    "    # Regex pattern for only alphanumeric, hyphenated text with 3 or more chars\n",
    "    pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\n",
    "    df['clean'] = df['reviewText'].str.findall(pattern).str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7af9cea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_no_stop</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>No adverse comment.</td>\n",
       "      <td>[no, adverse, comment]</td>\n",
       "      <td>[adverse, comment]</td>\n",
       "      <td>adverse comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Gift for college student.</td>\n",
       "      <td>[gift, for, college, student]</td>\n",
       "      <td>[gift, college, student]</td>\n",
       "      <td>Gift for college student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>If you like strong tea, this is for you. It mi...</td>\n",
       "      <td>[if, you, like, strong, tea, this, is, for, yo...</td>\n",
       "      <td>[like, strong, tea, little, strong]</td>\n",
       "      <td>you like strong tea this for you might even li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText  \\\n",
       "0      5.0                                No adverse comment.   \n",
       "1      5.0                          Gift for college student.   \n",
       "2      5.0  If you like strong tea, this is for you. It mi...   \n",
       "\n",
       "                                          text_split  \\\n",
       "0                             [no, adverse, comment]   \n",
       "1                      [gift, for, college, student]   \n",
       "2  [if, you, like, strong, tea, this, is, for, yo...   \n",
       "\n",
       "                          text_no_stop  \\\n",
       "0                   [adverse, comment]   \n",
       "1             [gift, college, student]   \n",
       "2  [like, strong, tea, little, strong]   \n",
       "\n",
       "                                               clean  \n",
       "0                                    adverse comment  \n",
       "1                           Gift for college student  \n",
       "2  you like strong tea this for you might even li...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preproc = cleaner(df_data)\n",
    "df_preproc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90228b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_pipe(doc):\n",
    "    lemma_list = [str(tok.lemma_).lower() for tok in doc\n",
    "                  if tok.is_alpha and tok.text.lower() not in stopwords] \n",
    "    return lemma_list\n",
    "\n",
    "def preprocess_pipe(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=20):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "    return preproc_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595796f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "babb63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"Flatten a list of lists to a combined list\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def process_chunk(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=20):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "    return preproc_pipe\n",
    "\n",
    "def preprocess_parallel(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(df_preproc), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e309b646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_no_stop</th>\n",
       "      <th>clean</th>\n",
       "      <th>preproc_pipe</th>\n",
       "      <th>preproc_parallel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>No adverse comment.</td>\n",
       "      <td>[no, adverse, comment]</td>\n",
       "      <td>[adverse, comment]</td>\n",
       "      <td>adverse comment</td>\n",
       "      <td>[adverse, comment]</td>\n",
       "      <td>[adverse, comment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Gift for college student.</td>\n",
       "      <td>[gift, for, college, student]</td>\n",
       "      <td>[gift, college, student]</td>\n",
       "      <td>Gift for college student</td>\n",
       "      <td>[gift, college, student]</td>\n",
       "      <td>[gift, college, student]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>If you like strong tea, this is for you. It mi...</td>\n",
       "      <td>[if, you, like, strong, tea, this, is, for, yo...</td>\n",
       "      <td>[like, strong, tea, little, strong]</td>\n",
       "      <td>you like strong tea this for you might even li...</td>\n",
       "      <td>[like, strong, tea, little, strong]</td>\n",
       "      <td>[like, strong, tea, little, strong]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText  \\\n",
       "0      5.0                                No adverse comment.   \n",
       "1      5.0                          Gift for college student.   \n",
       "2      5.0  If you like strong tea, this is for you. It mi...   \n",
       "\n",
       "                                          text_split  \\\n",
       "0                             [no, adverse, comment]   \n",
       "1                      [gift, for, college, student]   \n",
       "2  [if, you, like, strong, tea, this, is, for, yo...   \n",
       "\n",
       "                          text_no_stop  \\\n",
       "0                   [adverse, comment]   \n",
       "1             [gift, college, student]   \n",
       "2  [like, strong, tea, little, strong]   \n",
       "\n",
       "                                               clean  \\\n",
       "0                                    adverse comment   \n",
       "1                           Gift for college student   \n",
       "2  you like strong tea this for you might even li...   \n",
       "\n",
       "                          preproc_pipe                     preproc_parallel  \n",
       "0                   [adverse, comment]                   [adverse, comment]  \n",
       "1             [gift, college, student]             [gift, college, student]  \n",
       "2  [like, strong, tea, little, strong]  [like, strong, tea, little, strong]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preproc['preproc_parallel'] = preprocess_parallel(df_preproc['clean'], chunksize=1000)\n",
    "df_preproc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b5563de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use = df_preproc[[\"overall\", \"preproc_parallel\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab6c78d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>preproc_parallel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[adverse, comment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[gift, college, student]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[like, strong, tea, little, strong]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                     preproc_parallel\n",
       "0      5.0                   [adverse, comment]\n",
       "1      5.0             [gift, college, student]\n",
       "2      5.0  [like, strong, tea, little, strong]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_use.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18505e06",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30264/3570542758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create corpus and corresponding one-hot dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# something like:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preproc_parallel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "# create corpus and corresponding one-hot dictionary\n",
    "# something like:\n",
    "all_words = ' '.join([word for word in df_use['preproc_parallel']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57454f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [item for sublist in df_use['preproc_parallel'] for item in sublist]\n",
    "corpus = set(all_words)\n",
    "word_to_ix = {word: i for i, word in enumerate(corpus)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d5d8d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moister': 0,\n",
       " 'cigars': 1,\n",
       " 'linked': 2,\n",
       " 'colored': 3,\n",
       " 'blue': 4,\n",
       " 'ridiculously': 5,\n",
       " 'acidic': 6,\n",
       " 'subscribe': 7,\n",
       " 'stamped': 8,\n",
       " 'wasatch': 9,\n",
       " 'embarrassed': 10,\n",
       " 'awful': 11,\n",
       " 'party': 12,\n",
       " 'coronary': 13,\n",
       " 'regulation': 14,\n",
       " 'smoothies': 15,\n",
       " 'handful': 16,\n",
       " 'impact': 17,\n",
       " 'acids': 18,\n",
       " 'fundraiser': 19,\n",
       " 'guy': 20,\n",
       " 'conditions': 21,\n",
       " 'singing': 22,\n",
       " 'wanted': 23,\n",
       " 'poultry': 24,\n",
       " 'sell': 25,\n",
       " 'aim': 26,\n",
       " 'sign': 27,\n",
       " 'mocha': 28,\n",
       " 'stove': 29,\n",
       " 'experienced': 30,\n",
       " 'staled': 31,\n",
       " 'looks': 32,\n",
       " 'months': 33,\n",
       " 'static': 34,\n",
       " 'wild': 35,\n",
       " 'radiation': 36,\n",
       " 'ache': 37,\n",
       " 'life': 38,\n",
       " 'marshmallows': 39,\n",
       " 'alcohols': 40,\n",
       " 'carbs': 41,\n",
       " 'stars': 42,\n",
       " 'lime': 43,\n",
       " 'inverted': 44,\n",
       " 'words': 45,\n",
       " 'standby': 46,\n",
       " 'delicious': 47,\n",
       " 'flowing': 48,\n",
       " 'pollen': 49,\n",
       " 'years': 50,\n",
       " 'issue': 51,\n",
       " 'pull': 52,\n",
       " 'basil': 53,\n",
       " 'imho': 54,\n",
       " 'introduced': 55,\n",
       " 'shelves': 56,\n",
       " 'blew': 57,\n",
       " 'recipe': 58,\n",
       " 'economical': 59,\n",
       " 'rarely': 60,\n",
       " 'fancy': 61,\n",
       " 'enclosed': 62,\n",
       " 'granular': 63,\n",
       " 'occasion': 64,\n",
       " 'kamut': 65,\n",
       " 'straying': 66,\n",
       " 'extract': 67,\n",
       " 'thoughts': 68,\n",
       " 'wall': 69,\n",
       " 'snacks': 70,\n",
       " 'tbsp': 71,\n",
       " 'discount': 72,\n",
       " 'prevent': 73,\n",
       " 'flavour': 74,\n",
       " 'doubt': 75,\n",
       " 'percolator': 76,\n",
       " 'attached': 77,\n",
       " 'dispurse': 78,\n",
       " 'loose': 79,\n",
       " 'collegiate': 80,\n",
       " 'board': 81,\n",
       " 'saturated': 82,\n",
       " 'separate': 83,\n",
       " 'indicated': 84,\n",
       " 'wad': 85,\n",
       " 'vegetable': 86,\n",
       " 'metropolis': 87,\n",
       " 'weary': 88,\n",
       " 'mixing': 89,\n",
       " 'social': 90,\n",
       " 'essential': 91,\n",
       " 'exceeded': 92,\n",
       " 'pretty': 93,\n",
       " 'butter': 94,\n",
       " 'forward': 95,\n",
       " 'broccoli': 96,\n",
       " 'sinus': 97,\n",
       " 'select': 98,\n",
       " 'peventative': 99,\n",
       " 'golden': 100,\n",
       " 'patriotic': 101,\n",
       " 'starbucks': 102,\n",
       " 'popcorn': 103,\n",
       " 'magnifying': 104,\n",
       " 'duds': 105,\n",
       " 'enzyme': 106,\n",
       " 'grams': 107,\n",
       " 'momofuku': 108,\n",
       " 'inhale': 109,\n",
       " 'contains': 110,\n",
       " 'cons': 111,\n",
       " 'recommended': 112,\n",
       " 'na': 113,\n",
       " 'purse': 114,\n",
       " 'pleasant': 115,\n",
       " 'grain': 116,\n",
       " 'corn': 117,\n",
       " 'hss': 118,\n",
       " 'carry': 119,\n",
       " 'anithistamines': 120,\n",
       " 'roots': 121,\n",
       " 'honest': 122,\n",
       " 'replaced': 123,\n",
       " 'lets': 124,\n",
       " 'lighted': 125,\n",
       " 'improving': 126,\n",
       " 'costume': 127,\n",
       " 'couldln': 128,\n",
       " 'ruining': 129,\n",
       " 'garage': 130,\n",
       " 'smell': 131,\n",
       " 'rock': 132,\n",
       " 'despite': 133,\n",
       " 'bowl': 134,\n",
       " 'meringue': 135,\n",
       " 'getting': 136,\n",
       " 'tbs': 137,\n",
       " 'steep': 138,\n",
       " 'push': 139,\n",
       " 'sugary': 140,\n",
       " 'sooo': 141,\n",
       " 'bundt': 142,\n",
       " 'vegetables': 143,\n",
       " 'coffeemaker': 144,\n",
       " 'preteens': 145,\n",
       " 'breaks': 146,\n",
       " 'based': 147,\n",
       " 'gypsy': 148,\n",
       " 'fills': 149,\n",
       " 'memory': 150,\n",
       " 'lobby': 151,\n",
       " 'absolutely': 152,\n",
       " 'seller': 153,\n",
       " 'phuket': 154,\n",
       " 'calls': 155,\n",
       " 'presentation': 156,\n",
       " 'geweldige': 157,\n",
       " 'pot': 158,\n",
       " 'times': 159,\n",
       " 'successfully': 160,\n",
       " 'recommend': 161,\n",
       " 'difficult': 162,\n",
       " 'trend': 163,\n",
       " 'dried': 164,\n",
       " 'story': 165,\n",
       " 'diagnosed': 166,\n",
       " 'kind': 167,\n",
       " 'comming': 168,\n",
       " 'cast': 169,\n",
       " 'hygienist': 170,\n",
       " 'rust': 171,\n",
       " 'grab': 172,\n",
       " 'cleaning': 173,\n",
       " 'frig': 174,\n",
       " 'collection': 175,\n",
       " 'consistent': 176,\n",
       " 'pour': 177,\n",
       " 'y': 178,\n",
       " 'douse': 179,\n",
       " 'frosting': 180,\n",
       " 'tin': 181,\n",
       " 'instantly': 182,\n",
       " 'particles': 183,\n",
       " 'ketchup': 184,\n",
       " 'cakepops': 185,\n",
       " 'flaky': 186,\n",
       " 'subsequently': 187,\n",
       " 'dieting': 188,\n",
       " 'purchases': 189,\n",
       " 'anti': 190,\n",
       " 'choice': 191,\n",
       " 'pies': 192,\n",
       " 'chunks': 193,\n",
       " 'concerned': 194,\n",
       " 'bruce': 195,\n",
       " 'lifted': 196,\n",
       " 'louis': 197,\n",
       " 'rainbow': 198,\n",
       " 'pancakes': 199,\n",
       " 'tasteless': 200,\n",
       " 'box': 201,\n",
       " 'sensation': 202,\n",
       " 'packed': 203,\n",
       " 'likely': 204,\n",
       " 'items': 205,\n",
       " 'mean': 206,\n",
       " 'giant': 207,\n",
       " 'degree': 208,\n",
       " 'compromising': 209,\n",
       " 'park': 210,\n",
       " 'portions': 211,\n",
       " 'strip': 212,\n",
       " 'cold': 213,\n",
       " 'general': 214,\n",
       " 'produce': 215,\n",
       " 'sous': 216,\n",
       " 'powerbars': 217,\n",
       " 'damage': 218,\n",
       " 'score': 219,\n",
       " 'medicines': 220,\n",
       " 'yucky': 221,\n",
       " 'drinkers': 222,\n",
       " 'indian': 223,\n",
       " 'singapore': 224,\n",
       " 'easily': 225,\n",
       " 'powder': 226,\n",
       " 'konedog': 227,\n",
       " 'chocoholic': 228,\n",
       " 'biotene': 229,\n",
       " 'wheezy': 230,\n",
       " 'brit': 231,\n",
       " 'appreciate': 232,\n",
       " 'tolerate': 233,\n",
       " 'information': 234,\n",
       " 'according': 235,\n",
       " 'terribly': 236,\n",
       " 'melted': 237,\n",
       " 'biscuits': 238,\n",
       " 'spill': 239,\n",
       " 'ratio': 240,\n",
       " 'wise': 241,\n",
       " 'closer': 242,\n",
       " 'category': 243,\n",
       " 'force': 244,\n",
       " 'blood': 245,\n",
       " 'infection': 246,\n",
       " 'likers': 247,\n",
       " 'chocolate': 248,\n",
       " 'click': 249,\n",
       " 'portion': 250,\n",
       " 'kernel': 251,\n",
       " 'practically': 252,\n",
       " 'opened': 253,\n",
       " 'speckles': 254,\n",
       " 'cellophane': 255,\n",
       " 'kinds': 256,\n",
       " 'run': 257,\n",
       " 'g': 258,\n",
       " 'magic': 259,\n",
       " 'remainder': 260,\n",
       " 'awesome': 261,\n",
       " 'muddy': 262,\n",
       " 'impossible': 263,\n",
       " 'fave': 264,\n",
       " 'guzzling': 265,\n",
       " 'spoons': 266,\n",
       " 'appetite': 267,\n",
       " 'pasta': 268,\n",
       " 'chile': 269,\n",
       " 'hand': 270,\n",
       " 'spearmint': 271,\n",
       " 'gotten': 272,\n",
       " 'fingers': 273,\n",
       " 'messenger': 274,\n",
       " 'surprisingly': 275,\n",
       " 'brewing': 276,\n",
       " 'somebody': 277,\n",
       " 'acupuncture': 278,\n",
       " 'enjoyable': 279,\n",
       " 'received': 280,\n",
       " 'tickets': 281,\n",
       " 'coins': 282,\n",
       " 'bags': 283,\n",
       " 'coffees': 284,\n",
       " 'elated': 285,\n",
       " 'costly': 286,\n",
       " 'performance': 287,\n",
       " 'minimal': 288,\n",
       " 'actually': 289,\n",
       " 'interests': 290,\n",
       " 'ba': 291,\n",
       " 'packs': 292,\n",
       " 'ends': 293,\n",
       " 'afterlife': 294,\n",
       " 'meeting': 295,\n",
       " 'wat': 296,\n",
       " 'vertical': 297,\n",
       " 'combinations': 298,\n",
       " 'donuts': 299,\n",
       " 'thm': 300,\n",
       " 'standard': 301,\n",
       " 'imitation': 302,\n",
       " 'stinkin': 303,\n",
       " 'refer': 304,\n",
       " 'got': 305,\n",
       " 'aroma': 306,\n",
       " 'smash': 307,\n",
       " 'achingly': 308,\n",
       " 'shows': 309,\n",
       " 'basketball': 310,\n",
       " 'parmesan': 311,\n",
       " 'massive': 312,\n",
       " 'anniversary': 313,\n",
       " 'peterson': 314,\n",
       " 'stainless': 315,\n",
       " 'middle': 316,\n",
       " 'sodium': 317,\n",
       " 'panel': 318,\n",
       " 'descriptive': 319,\n",
       " 'beat': 320,\n",
       " 'buds': 321,\n",
       " 'bean': 322,\n",
       " 'stunning': 323,\n",
       " 'core': 324,\n",
       " 'brands': 325,\n",
       " 'handy': 326,\n",
       " 'cereal': 327,\n",
       " 'sold': 328,\n",
       " 'ideal': 329,\n",
       " 'organization': 330,\n",
       " 'wintermint': 331,\n",
       " 'length': 332,\n",
       " 'lasted': 333,\n",
       " 'lattes': 334,\n",
       " 'office': 335,\n",
       " 'blast': 336,\n",
       " 'famous': 337,\n",
       " 'achey': 338,\n",
       " 'bitterness': 339,\n",
       " 'naught': 340,\n",
       " 'phenominal': 341,\n",
       " 'iced': 342,\n",
       " 'reveals': 343,\n",
       " 'holidays': 344,\n",
       " 'stiffer': 345,\n",
       " 'tsp': 346,\n",
       " 'dentist': 347,\n",
       " 'kidding': 348,\n",
       " 'personally': 349,\n",
       " 'applesauce': 350,\n",
       " 'mass': 351,\n",
       " 'eat': 352,\n",
       " 'believing': 353,\n",
       " 'delights': 354,\n",
       " 'afternoon': 355,\n",
       " 'vareity': 356,\n",
       " 'decent': 357,\n",
       " 'bag': 358,\n",
       " 'growth': 359,\n",
       " 'generic': 360,\n",
       " 'assuming': 361,\n",
       " 'activities': 362,\n",
       " 'conditioner': 363,\n",
       " 'complicated': 364,\n",
       " 'repurchase': 365,\n",
       " 'exhaustion': 366,\n",
       " 'crap': 367,\n",
       " 'exposed': 368,\n",
       " 'stick': 369,\n",
       " 'meats': 370,\n",
       " 'denture': 371,\n",
       " 'reminds': 372,\n",
       " 'reduces': 373,\n",
       " 'king': 374,\n",
       " 'tall': 375,\n",
       " 'shaker': 376,\n",
       " 'national': 377,\n",
       " 'picked': 378,\n",
       " 'burn': 379,\n",
       " 'caution': 380,\n",
       " 'delightful': 381,\n",
       " 'nyc': 382,\n",
       " 'pirouline': 383,\n",
       " 'inches': 384,\n",
       " 'brown': 385,\n",
       " 'versatile': 386,\n",
       " 'evenly': 387,\n",
       " 'switched': 388,\n",
       " 'odd': 389,\n",
       " 'degrade': 390,\n",
       " 'suggested': 391,\n",
       " 'short': 392,\n",
       " 'units': 393,\n",
       " 'unexpected': 394,\n",
       " 'bed': 395,\n",
       " 'elastic': 396,\n",
       " 'carries': 397,\n",
       " 'patrons': 398,\n",
       " 'aromatic': 399,\n",
       " 'squash': 400,\n",
       " 'require': 401,\n",
       " 'bordering': 402,\n",
       " 'yep': 403,\n",
       " 'ways': 404,\n",
       " 'bars': 405,\n",
       " 'dollop': 406,\n",
       " 'fixation': 407,\n",
       " 'negative': 408,\n",
       " 'baking': 409,\n",
       " 'brings': 410,\n",
       " 'hunger': 411,\n",
       " 'bake': 412,\n",
       " 'grill': 413,\n",
       " 'herbal': 414,\n",
       " 'buck': 415,\n",
       " 'cups': 416,\n",
       " 'cutters': 417,\n",
       " 'order': 418,\n",
       " 'powerful': 419,\n",
       " 'vivd': 420,\n",
       " 'largest': 421,\n",
       " 'alot': 422,\n",
       " 'world': 423,\n",
       " 'ppermint': 424,\n",
       " 'mouths': 425,\n",
       " 'happy': 426,\n",
       " 'korn': 427,\n",
       " 'reading': 428,\n",
       " 'packaging': 429,\n",
       " 'love': 430,\n",
       " 'horrible': 431,\n",
       " 'lunch': 432,\n",
       " 'true': 433,\n",
       " 'sharp': 434,\n",
       " 'nearly': 435,\n",
       " 'month': 436,\n",
       " 'mixed': 437,\n",
       " 'spread': 438,\n",
       " 'assortment': 439,\n",
       " 'known': 440,\n",
       " 'replenished': 441,\n",
       " 'dim': 442,\n",
       " 'nutritionally': 443,\n",
       " 'topped': 444,\n",
       " 'causing': 445,\n",
       " 'dietician': 446,\n",
       " 'sharing': 447,\n",
       " 'term': 448,\n",
       " 'steel': 449,\n",
       " 'comment': 450,\n",
       " 'looking': 451,\n",
       " 'moved': 452,\n",
       " 'shopped': 453,\n",
       " 'gallon': 454,\n",
       " 'cancer': 455,\n",
       " 'remains': 456,\n",
       " 'apparatus': 457,\n",
       " 'forgot': 458,\n",
       " 'lots': 459,\n",
       " 'loaded': 460,\n",
       " 'aerobic': 461,\n",
       " 'sprinkles': 462,\n",
       " 'sloppy': 463,\n",
       " 'increase': 464,\n",
       " 'hershey': 465,\n",
       " 'sample': 466,\n",
       " 'ou': 467,\n",
       " 'molds': 468,\n",
       " 'taint': 469,\n",
       " 'freshness': 470,\n",
       " 'placed': 471,\n",
       " 'rule': 472,\n",
       " 'medications': 473,\n",
       " 'easy': 474,\n",
       " 'belgium': 475,\n",
       " 'mugs': 476,\n",
       " 'unedible': 477,\n",
       " 'yada': 478,\n",
       " 'grind': 479,\n",
       " 'goodies': 480,\n",
       " 'foundation': 481,\n",
       " 'coarsely': 482,\n",
       " 'arises': 483,\n",
       " 'gloves': 484,\n",
       " 'brownies': 485,\n",
       " 'sort': 486,\n",
       " 'cooked': 487,\n",
       " 'workable': 488,\n",
       " 'stuff': 489,\n",
       " 'retail': 490,\n",
       " 'deal': 491,\n",
       " 'college': 492,\n",
       " 'sprint': 493,\n",
       " 'crust': 494,\n",
       " 'york': 495,\n",
       " 'waffle': 496,\n",
       " 'speak': 497,\n",
       " 'worth': 498,\n",
       " 'fact': 499,\n",
       " 'clarified': 500,\n",
       " 'unlike': 501,\n",
       " 'supreme': 502,\n",
       " 'freshly': 503,\n",
       " 'worse': 504,\n",
       " 'cupcake': 505,\n",
       " 'yogurts': 506,\n",
       " 'shame': 507,\n",
       " 'nasty': 508,\n",
       " 'starbuck': 509,\n",
       " 'partially': 510,\n",
       " 'occurs': 511,\n",
       " 'harnesses': 512,\n",
       " 'strawberry': 513,\n",
       " 'featured': 514,\n",
       " 'earthy': 515,\n",
       " 'supplying': 516,\n",
       " 'chunk': 517,\n",
       " 'discovered': 518,\n",
       " 'bleed': 519,\n",
       " 'manufacture': 520,\n",
       " 'differences': 521,\n",
       " 'https': 522,\n",
       " 'casseroles': 523,\n",
       " 'sprinkle': 524,\n",
       " 'harder': 525,\n",
       " 'maxx': 526,\n",
       " 'throw': 527,\n",
       " 'defined': 528,\n",
       " 'adds': 529,\n",
       " 'whatsoever': 530,\n",
       " 'taablespoon': 531,\n",
       " 'worthwhile': 532,\n",
       " 'home': 533,\n",
       " 'alas': 534,\n",
       " 'cause': 535,\n",
       " 'factor': 536,\n",
       " 'com': 537,\n",
       " 'word': 538,\n",
       " 'exhaust': 539,\n",
       " 'action': 540,\n",
       " 'nostrils': 541,\n",
       " 'crispy': 542,\n",
       " 'covered': 543,\n",
       " 'allopathic': 544,\n",
       " 'stashes': 545,\n",
       " 'hook': 546,\n",
       " 'tends': 547,\n",
       " 'disgust': 548,\n",
       " 'randomly': 549,\n",
       " 'excluding': 550,\n",
       " 'single': 551,\n",
       " 'realized': 552,\n",
       " 'matte': 553,\n",
       " 'enjoys': 554,\n",
       " 'divvy': 555,\n",
       " 'raisins': 556,\n",
       " 'doctor': 557,\n",
       " 'liquor': 558,\n",
       " 'recognizable': 559,\n",
       " 'merry': 560,\n",
       " 'provided': 561,\n",
       " 'secret': 562,\n",
       " 'supposed': 563,\n",
       " 'sisters': 564,\n",
       " 'thank': 565,\n",
       " 'possible': 566,\n",
       " 'coffe': 567,\n",
       " 'nestle': 568,\n",
       " 'replace': 569,\n",
       " 'numbers': 570,\n",
       " 'released': 571,\n",
       " 'starchy': 572,\n",
       " 'purpose': 573,\n",
       " 'therapy': 574,\n",
       " 'piece': 575,\n",
       " 'preparation': 576,\n",
       " 'flow': 577,\n",
       " 'serves': 578,\n",
       " 'halitosis': 579,\n",
       " 'lightly': 580,\n",
       " 'wafer': 581,\n",
       " 'practice': 582,\n",
       " 'flu': 583,\n",
       " 'orange': 584,\n",
       " 'eaten': 585,\n",
       " 'mimics': 586,\n",
       " 'officially': 587,\n",
       " 'fyi': 588,\n",
       " 'riding': 589,\n",
       " 'absorbs': 590,\n",
       " 'oreos': 591,\n",
       " 'grocery': 592,\n",
       " 'kah': 593,\n",
       " 'penetrate': 594,\n",
       " 'pristine': 595,\n",
       " 'original': 596,\n",
       " 'volume': 597,\n",
       " 'dame': 598,\n",
       " 'processes': 599,\n",
       " 'mornings': 600,\n",
       " 'minutes': 601,\n",
       " 'complex': 602,\n",
       " 'dates': 603,\n",
       " 'bashed': 604,\n",
       " 'batch': 605,\n",
       " 'frying': 606,\n",
       " 'swept': 607,\n",
       " 'ca': 608,\n",
       " 'jar': 609,\n",
       " 'rigs': 610,\n",
       " 'craving': 611,\n",
       " 'ranging': 612,\n",
       " 'perfumes': 613,\n",
       " 'special': 614,\n",
       " 'iron': 615,\n",
       " 'stretched': 616,\n",
       " 'friend': 617,\n",
       " 'clif': 618,\n",
       " 'gmo': 619,\n",
       " 'sticker': 620,\n",
       " 'power': 621,\n",
       " 'minute': 622,\n",
       " 'level': 623,\n",
       " 'freezing': 624,\n",
       " 'poaching': 625,\n",
       " 'exchange': 626,\n",
       " 'adores': 627,\n",
       " 'spatula': 628,\n",
       " 'banana': 629,\n",
       " 'cupboards': 630,\n",
       " 'walks': 631,\n",
       " 'newbie': 632,\n",
       " 'decafe': 633,\n",
       " 'java': 634,\n",
       " 'effect': 635,\n",
       " 'infections': 636,\n",
       " 'freak': 637,\n",
       " 'expect': 638,\n",
       " 'line': 639,\n",
       " 'mind': 640,\n",
       " 'combat': 641,\n",
       " 'barley': 642,\n",
       " 'aspects': 643,\n",
       " 'things': 644,\n",
       " 'says': 645,\n",
       " 'seasonal': 646,\n",
       " 'creamer': 647,\n",
       " 'tee': 648,\n",
       " 'gourmet': 649,\n",
       " 'xmas': 650,\n",
       " 'offering': 651,\n",
       " 'imagination': 652,\n",
       " 'valilla': 653,\n",
       " 'jerky': 654,\n",
       " 'trading': 655,\n",
       " 'die': 656,\n",
       " 'egderts': 657,\n",
       " 'sticking': 658,\n",
       " 'stains': 659,\n",
       " 'nearing': 660,\n",
       " 'decide': 661,\n",
       " 'bend': 662,\n",
       " 'cases': 663,\n",
       " 'reviewers': 664,\n",
       " 'powering': 665,\n",
       " 'maybe': 666,\n",
       " 'sugars': 667,\n",
       " 'slender': 668,\n",
       " 'measuring': 669,\n",
       " 'semi': 670,\n",
       " 'pasted': 671,\n",
       " 'sounds': 672,\n",
       " 'tortellini': 673,\n",
       " 'breaking': 674,\n",
       " 'giving': 675,\n",
       " 'clog': 676,\n",
       " 'folate': 677,\n",
       " 'brand': 678,\n",
       " 'curious': 679,\n",
       " 'snow': 680,\n",
       " 'potent': 681,\n",
       " 'rec': 682,\n",
       " 'fruity': 683,\n",
       " 'congestions': 684,\n",
       " 'ingredient': 685,\n",
       " 'pack': 686,\n",
       " 'chef': 687,\n",
       " 'year': 688,\n",
       " 'industrial': 689,\n",
       " 'questions': 690,\n",
       " 'oat': 691,\n",
       " 'vanishes': 692,\n",
       " 'stone': 693,\n",
       " 'condensed': 694,\n",
       " 'pills': 695,\n",
       " 'blends': 696,\n",
       " 'serve': 697,\n",
       " 'neat': 698,\n",
       " 'cleaner': 699,\n",
       " 'op': 700,\n",
       " 'portable': 701,\n",
       " 'vitamin': 702,\n",
       " 'pants': 703,\n",
       " 'currently': 704,\n",
       " 'columbian': 705,\n",
       " 'metal': 706,\n",
       " 'frost': 707,\n",
       " 'classmates': 708,\n",
       " 'fajita': 709,\n",
       " 'think': 710,\n",
       " 'explained': 711,\n",
       " 'watery': 712,\n",
       " 'highly': 713,\n",
       " 'convenient': 714,\n",
       " 'pans': 715,\n",
       " 'wonderful': 716,\n",
       " 'illusion': 717,\n",
       " 'saw': 718,\n",
       " 'plenty': 719,\n",
       " 'protect': 720,\n",
       " 'healthy': 721,\n",
       " 'follow': 722,\n",
       " 'walk': 723,\n",
       " 'blend': 724,\n",
       " 'young': 725,\n",
       " 'smelling': 726,\n",
       " 'bought': 727,\n",
       " 'definite': 728,\n",
       " 'news': 729,\n",
       " 'current': 730,\n",
       " 'goes': 731,\n",
       " 'linden': 732,\n",
       " 'gifting': 733,\n",
       " 'clean': 734,\n",
       " 'rest': 735,\n",
       " 'zing': 736,\n",
       " 'purchase': 737,\n",
       " 'minors': 738,\n",
       " 'saltiness': 739,\n",
       " 'stencil': 740,\n",
       " 'addictions': 741,\n",
       " 'mints': 742,\n",
       " 'mouth': 743,\n",
       " 'asking': 744,\n",
       " 'happen': 745,\n",
       " 'generally': 746,\n",
       " 'hanging': 747,\n",
       " 'tone': 748,\n",
       " 'eacute': 749,\n",
       " 'handling': 750,\n",
       " 'thyme': 751,\n",
       " 'quite': 752,\n",
       " 'joe': 753,\n",
       " 'gets': 754,\n",
       " 'lunches': 755,\n",
       " 'soooo': 756,\n",
       " 'easiest': 757,\n",
       " 'enjoying': 758,\n",
       " 'cooled': 759,\n",
       " 'seeing': 760,\n",
       " 'ratings': 761,\n",
       " 'personal': 762,\n",
       " 'palate': 763,\n",
       " 'covers': 764,\n",
       " 'feeding': 765,\n",
       " 'specially': 766,\n",
       " 'read': 767,\n",
       " 'starting': 768,\n",
       " 'phenomenal': 769,\n",
       " 'ethnic': 770,\n",
       " 'attacks': 771,\n",
       " 'incredible': 772,\n",
       " 'prices': 773,\n",
       " 'preservatives': 774,\n",
       " 'return': 775,\n",
       " 'badly': 776,\n",
       " 'matched': 777,\n",
       " 'blow': 778,\n",
       " 'cinnamint': 779,\n",
       " 'ball': 780,\n",
       " 'istanbul': 781,\n",
       " 'spaghetti': 782,\n",
       " 'paul': 783,\n",
       " 'starch': 784,\n",
       " 'report': 785,\n",
       " 'began': 786,\n",
       " 'sick': 787,\n",
       " 'tub': 788,\n",
       " 'shoots': 789,\n",
       " 'spend': 790,\n",
       " 'effective': 791,\n",
       " 'shinny': 792,\n",
       " 'talking': 793,\n",
       " 'wondered': 794,\n",
       " 'smells': 795,\n",
       " 'don': 796,\n",
       " 'reusable': 797,\n",
       " 'thing': 798,\n",
       " 'frickin': 799,\n",
       " 'creating': 800,\n",
       " 'stocked': 801,\n",
       " 'generous': 802,\n",
       " 'berries': 803,\n",
       " 'vegan': 804,\n",
       " 'discriminating': 805,\n",
       " 'simple': 806,\n",
       " 'tangy': 807,\n",
       " 'strong': 808,\n",
       " 'potassium': 809,\n",
       " 'traveling': 810,\n",
       " 'ragnar': 811,\n",
       " 'occasions': 812,\n",
       " 'tax': 813,\n",
       " 'afghanistan': 814,\n",
       " 'alcohol': 815,\n",
       " 'chloride': 816,\n",
       " 'bear': 817,\n",
       " 'protected': 818,\n",
       " 'nonsense': 819,\n",
       " 'icky': 820,\n",
       " 'caveats': 821,\n",
       " 'pure': 822,\n",
       " 'transform': 823,\n",
       " 'carbonate': 824,\n",
       " 'weren': 825,\n",
       " 'form': 826,\n",
       " 'shut': 827,\n",
       " 'settled': 828,\n",
       " 'cake': 829,\n",
       " 'early': 830,\n",
       " 'kusmi': 831,\n",
       " 'loves': 832,\n",
       " 'experiment': 833,\n",
       " 'malaysia': 834,\n",
       " 'cousin': 835,\n",
       " 'tacky': 836,\n",
       " 'hates': 837,\n",
       " 'strenuous': 838,\n",
       " 'strips': 839,\n",
       " 'efficacy': 840,\n",
       " 'sitting': 841,\n",
       " 'regimen': 842,\n",
       " 'premium': 843,\n",
       " 'photo': 844,\n",
       " 'achieve': 845,\n",
       " 'contained': 846,\n",
       " 'lasts': 847,\n",
       " 'sorely': 848,\n",
       " 'cheddars': 849,\n",
       " 'crushed': 850,\n",
       " 'hundreds': 851,\n",
       " 'carbon': 852,\n",
       " 'infused': 853,\n",
       " 'trans': 854,\n",
       " 'like': 855,\n",
       " 'freshest': 856,\n",
       " 'quarter': 857,\n",
       " 'raised': 858,\n",
       " 'success': 859,\n",
       " 'elements': 860,\n",
       " 'going': 861,\n",
       " 'mold': 862,\n",
       " 'releasing': 863,\n",
       " 'rico': 864,\n",
       " 'tightly': 865,\n",
       " 'identification': 866,\n",
       " 'sinuses': 867,\n",
       " 'grandson': 868,\n",
       " 'pudding': 869,\n",
       " 'mom': 870,\n",
       " 'sparingly': 871,\n",
       " 'candy': 872,\n",
       " 'menthol': 873,\n",
       " 'methods': 874,\n",
       " 'start': 875,\n",
       " 'stickiness': 876,\n",
       " 'someones': 877,\n",
       " 'spicy': 878,\n",
       " 'lovely': 879,\n",
       " 'checked': 880,\n",
       " 'sized': 881,\n",
       " 'sprays': 882,\n",
       " 'sticks': 883,\n",
       " 'grape': 884,\n",
       " 'tines': 885,\n",
       " 'rich': 886,\n",
       " 'norwegian': 887,\n",
       " 'reliable': 888,\n",
       " 'santa': 889,\n",
       " 'mighty': 890,\n",
       " 'favorable': 891,\n",
       " 'desserts': 892,\n",
       " 'macarons': 893,\n",
       " 'meh': 894,\n",
       " 'compliments': 895,\n",
       " 'dishes': 896,\n",
       " 'flavoring': 897,\n",
       " 'improvement': 898,\n",
       " 'bucks': 899,\n",
       " 'ordered': 900,\n",
       " 'predominantly': 901,\n",
       " 'real': 902,\n",
       " 'lacking': 903,\n",
       " 'awhile': 904,\n",
       " 'soft': 905,\n",
       " 'patients': 906,\n",
       " 'apart': 907,\n",
       " 'significant': 908,\n",
       " 'beginning': 909,\n",
       " 'gum': 910,\n",
       " 'overspray': 911,\n",
       " 'boost': 912,\n",
       " 'carton': 913,\n",
       " 'lot': 914,\n",
       " 'artificial': 915,\n",
       " 'hour': 916,\n",
       " 'outlasts': 917,\n",
       " 'lactose': 918,\n",
       " 'clogging': 919,\n",
       " 'grandkids': 920,\n",
       " 'notes': 921,\n",
       " 'confused': 922,\n",
       " 'tossed': 923,\n",
       " 'indulging': 924,\n",
       " 'liked': 925,\n",
       " 'product': 926,\n",
       " 'fell': 927,\n",
       " 'thai': 928,\n",
       " 'western': 929,\n",
       " 'dilmah': 930,\n",
       " 'burns': 931,\n",
       " 'pucker': 932,\n",
       " 'means': 933,\n",
       " 'koenig': 934,\n",
       " 'causes': 935,\n",
       " 'helpful': 936,\n",
       " 'fillers': 937,\n",
       " 'person': 938,\n",
       " 'threw': 939,\n",
       " 'consult': 940,\n",
       " 'exercising': 941,\n",
       " 'compares': 942,\n",
       " 'benefits': 943,\n",
       " 'additional': 944,\n",
       " 'oretty': 945,\n",
       " 'answer': 946,\n",
       " 'blended': 947,\n",
       " 'distinct': 948,\n",
       " 'spoon': 949,\n",
       " 'cosr': 950,\n",
       " 'easing': 951,\n",
       " 'couple': 952,\n",
       " 'overpoweringly': 953,\n",
       " 'kisses': 954,\n",
       " 'compulsive': 955,\n",
       " 'slight': 956,\n",
       " 'mainly': 957,\n",
       " 'adult': 958,\n",
       " 'cost': 959,\n",
       " 'munch': 960,\n",
       " 'activity': 961,\n",
       " 'pondering': 962,\n",
       " 'arp': 963,\n",
       " 'chocoholics': 964,\n",
       " 'eden': 965,\n",
       " 'annie': 966,\n",
       " 'compare': 967,\n",
       " 'lasting': 968,\n",
       " 'wide': 969,\n",
       " 'exists': 970,\n",
       " 'wrapper': 971,\n",
       " 'anticipated': 972,\n",
       " 'seed': 973,\n",
       " 'breathe': 974,\n",
       " 'curried': 975,\n",
       " 'website': 976,\n",
       " 'runs': 977,\n",
       " 'migraines': 978,\n",
       " 'replenish': 979,\n",
       " 'treatment': 980,\n",
       " 'roasts': 981,\n",
       " 'relatively': 982,\n",
       " 'tummy': 983,\n",
       " 'bubbles': 984,\n",
       " 'clinging': 985,\n",
       " 'say': 986,\n",
       " 'tomb': 987,\n",
       " 'satisfactory': 988,\n",
       " 'strengths': 989,\n",
       " 'wearers': 990,\n",
       " 'marinades': 991,\n",
       " 'weird': 992,\n",
       " 'roll': 993,\n",
       " 'porch': 994,\n",
       " 'literally': 995,\n",
       " 'waxed': 996,\n",
       " 'science': 997,\n",
       " 'hobby': 998,\n",
       " 'definition': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71cc66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', 244), ('flavor', 239), ('good', 236), ('like', 234), ('gum', 159), ('love', 152), ('tea', 145), ('use', 142), ('taste', 141), ('just', 138)]\n"
     ]
    }
   ],
   "source": [
    "# 10 most common words? \n",
    "from collections import Counter\n",
    "\n",
    "Counter = Counter(all_words)\n",
    "  \n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter.most_common(10)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00cb9ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdc0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7874b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ff7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

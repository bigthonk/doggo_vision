{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88baf26a",
   "metadata": {},
   "source": [
    "# Build a model 2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135ff5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.feature_extraction import text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e57ff2",
   "metadata": {},
   "source": [
    "## Read functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a09756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825c5db",
   "metadata": {},
   "source": [
    "## Clean and preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2da274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(df):\n",
    "    \"Extract relevant text from DataFrame using a regex\"\n",
    "    # Regex pattern for only alphanumeric, hyphenated text with 3 or more chars\n",
    "    pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\n",
    "    df['clean'] = df['reviewText'].str.findall(pattern).str.join(' ')\n",
    "    return df\n",
    "\n",
    "def lemmatize_pipe(doc):\n",
    "    lemma_list = [str(tok.lemma_).lower() for tok in doc\n",
    "                  if tok.is_alpha and tok.text.lower() not in stopwords] \n",
    "    return lemma_list\n",
    "\n",
    "def preprocess_pipe(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=20):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "    return preproc_pipe\n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"Flatten a list of lists to a combined list\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def process_chunk(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=20):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "    return preproc_pipe\n",
    "\n",
    "def preprocess_parallel(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(df_clean), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278afc12",
   "metadata": {},
   "source": [
    "## Corpus and one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde5dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff6ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25ebd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[adverse, comment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[gift, college, student]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[like, strong, tea, little, strong]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                 text\n",
       "0      5.0                   [adverse, comment]\n",
       "1      5.0             [gift, college, student]\n",
       "2      5.0  [like, strong, tea, little, strong]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_limit = 1000\n",
    "set_chunksize = 1000\n",
    "\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df = getDF('Grocery_and_Gourmet_Food_5.json.gz')\n",
    "df_limit = df.head(n_limit)\n",
    "df_clean = cleaner(df_limit)\n",
    "\n",
    "df_clean['text'] = preprocess_parallel(df_clean['clean'], chunksize=1000)\n",
    "\n",
    "df_clean = df_clean[[\"overall\", \"text\"]]\n",
    "\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e699b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 301), ('flavor', 290), ('use', 289), ('like', 254), ('great', 244), ('taste', 226), ('love', 185), ('gum', 180), ('buy', 160), ('tea', 159)]\n"
     ]
    }
   ],
   "source": [
    "all_words = [item for sublist in df_clean['text'] for item in sublist]\n",
    "corpus = set(all_words)\n",
    "word_to_ix = {word: i for i, word in enumerate(corpus)}\n",
    "\n",
    "n_corpus = len(corpus)\n",
    "\n",
    "Counter = Counter(all_words)\n",
    "  \n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter.most_common(10)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce0bfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foam': 0,\n",
       " 'powdered': 1,\n",
       " 'original': 2,\n",
       " 'bars': 3,\n",
       " 'antihistamine': 4,\n",
       " 'tanginess': 5,\n",
       " 'reach': 6,\n",
       " 'find': 7,\n",
       " 'rs': 8,\n",
       " 'sight': 9,\n",
       " 'aromatic': 10,\n",
       " 'precious': 11,\n",
       " 'spelt': 12,\n",
       " 'hand': 13,\n",
       " 'static': 14,\n",
       " 'deepness': 15,\n",
       " 'genisoy': 16,\n",
       " 'reliably': 17,\n",
       " 'fingertip': 18,\n",
       " 'aid': 19,\n",
       " 'sense': 20,\n",
       " 'sprayed': 21,\n",
       " 'med': 22,\n",
       " 'ready': 23,\n",
       " 'swiss': 24,\n",
       " 'dull': 25,\n",
       " 'sunday': 26,\n",
       " 'improve': 27,\n",
       " 'manufacturing': 28,\n",
       " 'bakery': 29,\n",
       " 'trouble': 30,\n",
       " 'interest': 31,\n",
       " 'accomplish': 32,\n",
       " 'automobile': 33,\n",
       " 'immunity': 34,\n",
       " 'loyal': 35,\n",
       " 'combo': 36,\n",
       " 'session': 37,\n",
       " 'drop': 38,\n",
       " 'funky': 39,\n",
       " 'forever': 40,\n",
       " 'part': 41,\n",
       " 'delivery': 42,\n",
       " 'nesco': 43,\n",
       " 'portable': 44,\n",
       " 'pink': 45,\n",
       " 'relatively': 46,\n",
       " 'quick': 47,\n",
       " 'istanbul': 48,\n",
       " 'hispanics': 49,\n",
       " 'massive': 50,\n",
       " 'impress': 51,\n",
       " 'sweeten': 52,\n",
       " 'mac': 53,\n",
       " 'chain': 54,\n",
       " 'noodle': 55,\n",
       " 'wakes': 56,\n",
       " 'introduce': 57,\n",
       " 'halloween': 58,\n",
       " 'bag': 59,\n",
       " 'dough': 60,\n",
       " 'hungry': 61,\n",
       " 'potassium': 62,\n",
       " 'wished': 63,\n",
       " 'do': 64,\n",
       " 'bubblegum': 65,\n",
       " 'omnomnomnom': 66,\n",
       " 'roasting': 67,\n",
       " 'sign': 68,\n",
       " 'efficiently': 69,\n",
       " 'grandkid': 70,\n",
       " 'gel': 71,\n",
       " 'aisle': 72,\n",
       " 'great': 73,\n",
       " 'way': 74,\n",
       " 'near': 75,\n",
       " 'twice': 76,\n",
       " 'life': 77,\n",
       " 'youth': 78,\n",
       " 'afghanistan': 79,\n",
       " 'amazment': 80,\n",
       " 'avoid': 81,\n",
       " 'falutin': 82,\n",
       " 'production': 83,\n",
       " 'car': 84,\n",
       " 'citrus': 85,\n",
       " 'prime': 86,\n",
       " 'bit': 87,\n",
       " 'seriously': 88,\n",
       " 'extract': 89,\n",
       " 'worthy': 90,\n",
       " 'maltodextrin': 91,\n",
       " 'pleased': 92,\n",
       " 'vanilla': 93,\n",
       " 'economical': 94,\n",
       " 'dollar': 95,\n",
       " 'develop': 96,\n",
       " 'ombre': 97,\n",
       " 'seasoning': 98,\n",
       " 'hide': 99,\n",
       " 'briel': 100,\n",
       " 'relay': 101,\n",
       " 'eventually': 102,\n",
       " 'subdued': 103,\n",
       " 'selling': 104,\n",
       " 'fold': 105,\n",
       " 'stream': 106,\n",
       " 'highly': 107,\n",
       " 'vivd': 108,\n",
       " 'requirement': 109,\n",
       " 'likely': 110,\n",
       " 'microwave': 111,\n",
       " 'scramble': 112,\n",
       " 'suppressant': 113,\n",
       " 'someone': 114,\n",
       " 'mushroom': 115,\n",
       " 'struggle': 116,\n",
       " 'optimistic': 117,\n",
       " 'dreadful': 118,\n",
       " 'dunkin': 119,\n",
       " 'complete': 120,\n",
       " 'freezing': 121,\n",
       " 'mistake': 122,\n",
       " 'call': 123,\n",
       " 'unless': 124,\n",
       " 'dispensing': 125,\n",
       " 'snap': 126,\n",
       " 'fairly': 127,\n",
       " 'spill': 128,\n",
       " 'decay': 129,\n",
       " 'niacin': 130,\n",
       " 'enclose': 131,\n",
       " 'watch': 132,\n",
       " 'sprinkled': 133,\n",
       " 'drain': 134,\n",
       " 'imho': 135,\n",
       " 'technically': 136,\n",
       " 'liquor': 137,\n",
       " 'hotter': 138,\n",
       " 'shrimp': 139,\n",
       " 'glocery': 140,\n",
       " 'disgust': 141,\n",
       " 'timely': 142,\n",
       " 'dirty': 143,\n",
       " 'bleeding': 144,\n",
       " 'really': 145,\n",
       " 'primary': 146,\n",
       " 'hassle': 147,\n",
       " 'hail': 148,\n",
       " 'kiss': 149,\n",
       " 'additive': 150,\n",
       " 'kinda': 151,\n",
       " 'passage': 152,\n",
       " 'punchy': 153,\n",
       " 'mud': 154,\n",
       " 'guru': 155,\n",
       " 'swallow': 156,\n",
       " 'appeal': 157,\n",
       " 'convenient': 158,\n",
       " 'towel': 159,\n",
       " 'freshness': 160,\n",
       " 'chaching': 161,\n",
       " 'social': 162,\n",
       " 'run': 163,\n",
       " 'aspartame': 164,\n",
       " 'try': 165,\n",
       " 'flu': 166,\n",
       " 'hurry': 167,\n",
       " 'thing': 168,\n",
       " 'private': 169,\n",
       " 'fruity': 170,\n",
       " 'clog': 171,\n",
       " 'cafe': 172,\n",
       " 'randomly': 173,\n",
       " 'chew': 174,\n",
       " 'spicey': 175,\n",
       " 'carbohydrates': 176,\n",
       " 'throw': 177,\n",
       " 'dieting': 178,\n",
       " 'avid': 179,\n",
       " 'meat': 180,\n",
       " 'barbecue': 181,\n",
       " 'love': 182,\n",
       " 'arrives': 183,\n",
       " 'fast': 184,\n",
       " 'fluffy': 185,\n",
       " 'ref': 186,\n",
       " 'heavy': 187,\n",
       " 'dietician': 188,\n",
       " 'chewy': 189,\n",
       " 'road': 190,\n",
       " 'kid': 191,\n",
       " 'brit': 192,\n",
       " 'gazillion': 193,\n",
       " 'downside': 194,\n",
       " 'palmitic': 195,\n",
       " 'tamp': 196,\n",
       " 'authentic': 197,\n",
       " 'recommendation': 198,\n",
       " 'slender': 199,\n",
       " 'tastey': 200,\n",
       " 'amazon': 201,\n",
       " 'belgium': 202,\n",
       " 'pleasantly': 203,\n",
       " 'rating': 204,\n",
       " 'encounter': 205,\n",
       " 'pros': 206,\n",
       " 'instruction': 207,\n",
       " 'mega': 208,\n",
       " 'abby': 209,\n",
       " 'frig': 210,\n",
       " 'glycerin': 211,\n",
       " 'story': 212,\n",
       " 'dentine': 213,\n",
       " 'kernel': 214,\n",
       " 'nearby': 215,\n",
       " 'wow': 216,\n",
       " 'tend': 217,\n",
       " 'pudding': 218,\n",
       " 'slice': 219,\n",
       " 'tweak': 220,\n",
       " 'solution': 221,\n",
       " 'fan': 222,\n",
       " 'fail': 223,\n",
       " 'magnifying': 224,\n",
       " 'effective': 225,\n",
       " 'shelf': 226,\n",
       " 'stiff': 227,\n",
       " 'saturate': 228,\n",
       " 'halve': 229,\n",
       " 'paste': 230,\n",
       " 'pointless': 231,\n",
       " 'take': 232,\n",
       " 'non': 233,\n",
       " 'alternative': 234,\n",
       " 'lifestyle': 235,\n",
       " 'trip': 236,\n",
       " 'zucchini': 237,\n",
       " 'classic': 238,\n",
       " 'wintermint': 239,\n",
       " 'college': 240,\n",
       " 'inspiration': 241,\n",
       " 'chance': 242,\n",
       " 'large': 243,\n",
       " 'think': 244,\n",
       " 'tap': 245,\n",
       " 'carbs': 246,\n",
       " 'bodied': 247,\n",
       " 'supply': 248,\n",
       " 'hue': 249,\n",
       " 'reveal': 250,\n",
       " 'organization': 251,\n",
       " 'random': 252,\n",
       " 'order': 253,\n",
       " 'stop': 254,\n",
       " 'heartily': 255,\n",
       " 'lamp': 256,\n",
       " 'hole': 257,\n",
       " 'runner': 258,\n",
       " 'got': 259,\n",
       " 'throat': 260,\n",
       " 'fortune': 261,\n",
       " 'stash': 262,\n",
       " 'troop': 263,\n",
       " 'perfection': 264,\n",
       " 'house': 265,\n",
       " 'grant': 266,\n",
       " 'flow': 267,\n",
       " 'jar': 268,\n",
       " 'wrapper': 269,\n",
       " 'phosphorus': 270,\n",
       " 'girl': 271,\n",
       " 'get': 272,\n",
       " 'wheezy': 273,\n",
       " 'smash': 274,\n",
       " 'disappointment': 275,\n",
       " 'cook': 276,\n",
       " 'improvement': 277,\n",
       " 'cookie': 278,\n",
       " 'pieces': 279,\n",
       " 'brew': 280,\n",
       " 'plethora': 281,\n",
       " 'lime': 282,\n",
       " 'iwill': 283,\n",
       " 'diahrrea': 284,\n",
       " 'sea': 285,\n",
       " 'rico': 286,\n",
       " 'plate': 287,\n",
       " 'tolerate': 288,\n",
       " 'have': 289,\n",
       " 'peterson': 290,\n",
       " 'nice': 291,\n",
       " 'satisfie': 292,\n",
       " 'breast': 293,\n",
       " 'egberts': 294,\n",
       " 'hubby': 295,\n",
       " 'fatigue': 296,\n",
       " 'cup': 297,\n",
       " 'idea': 298,\n",
       " 'lose': 299,\n",
       " 'pump': 300,\n",
       " 'teabag': 301,\n",
       " 'impression': 302,\n",
       " 'turkey': 303,\n",
       " 'action': 304,\n",
       " 'virus': 305,\n",
       " 'association': 306,\n",
       " 'nightmare': 307,\n",
       " 'finger': 308,\n",
       " 'skip': 309,\n",
       " 'hydrogenate': 310,\n",
       " 'steel': 311,\n",
       " 'usa': 312,\n",
       " 'dixie': 313,\n",
       " 'lazy': 314,\n",
       " 'macaron': 315,\n",
       " 'honest': 316,\n",
       " 'chart': 317,\n",
       " 'specially': 318,\n",
       " 'feed': 319,\n",
       " 'essentially': 320,\n",
       " 'residue': 321,\n",
       " 'invert': 322,\n",
       " 'drip': 323,\n",
       " 'momofuku': 324,\n",
       " 'paleo': 325,\n",
       " 'acid': 326,\n",
       " 'war': 327,\n",
       " 'filling': 328,\n",
       " 'stunning': 329,\n",
       " 'tangy': 330,\n",
       " 'eastern': 331,\n",
       " 'cousin': 332,\n",
       " 'happy': 333,\n",
       " 'fry': 334,\n",
       " 'vaguely': 335,\n",
       " 'heartburn': 336,\n",
       " 'traditions': 337,\n",
       " 'afterlife': 338,\n",
       " 'coffeemaker': 339,\n",
       " 'collection': 340,\n",
       " 'splenda': 341,\n",
       " 'versatile': 342,\n",
       " 'grow': 343,\n",
       " 'messenger': 344,\n",
       " 'makes': 345,\n",
       " 'friendly': 346,\n",
       " 'casino': 347,\n",
       " 'gifted': 348,\n",
       " 'underneath': 349,\n",
       " 'fat': 350,\n",
       " 'anyother': 351,\n",
       " 'underlie': 352,\n",
       " 'nasty': 353,\n",
       " 'tube': 354,\n",
       " 'condensed': 355,\n",
       " 'usually': 356,\n",
       " 'pot': 357,\n",
       " 'increased': 358,\n",
       " 'soup': 359,\n",
       " 'entirely': 360,\n",
       " 'rarely': 361,\n",
       " 'wonderfully': 362,\n",
       " 'pallet': 363,\n",
       " 'sliced': 364,\n",
       " 'colored': 365,\n",
       " 'appetite': 366,\n",
       " 'harness': 367,\n",
       " 'peppermint': 368,\n",
       " 'oven': 369,\n",
       " 'marvelous': 370,\n",
       " 'thingy': 371,\n",
       " 'stay': 372,\n",
       " 'unlike': 373,\n",
       " 'beautiful': 374,\n",
       " 'nutritionally': 375,\n",
       " 'stamped': 376,\n",
       " 'stir': 377,\n",
       " 'sweep': 378,\n",
       " 'pleasant': 379,\n",
       " 'noticed': 380,\n",
       " 'nut': 381,\n",
       " 'awful': 382,\n",
       " 'lovely': 383,\n",
       " 'egg': 384,\n",
       " 'freak': 385,\n",
       " 'isn': 386,\n",
       " 'talk': 387,\n",
       " 'moist': 388,\n",
       " 'possible': 389,\n",
       " 'surprise': 390,\n",
       " 'hadn': 391,\n",
       " 'rice': 392,\n",
       " 'enjoy': 393,\n",
       " 'meantime': 394,\n",
       " 'bruce': 395,\n",
       " 'happily': 396,\n",
       " 'cellophane': 397,\n",
       " 'term': 398,\n",
       " 'stray': 399,\n",
       " 'ignition': 400,\n",
       " 'comment': 401,\n",
       " 'witness': 402,\n",
       " 'famous': 403,\n",
       " 'williams': 404,\n",
       " 'women': 405,\n",
       " 'able': 406,\n",
       " 'satisfaction': 407,\n",
       " 'migraine': 408,\n",
       " 'flavored': 409,\n",
       " 'few': 410,\n",
       " 'saliva': 411,\n",
       " 'exhaust': 412,\n",
       " 'errand': 413,\n",
       " 'mean': 414,\n",
       " 'pear': 415,\n",
       " 'site': 416,\n",
       " 'helped': 417,\n",
       " 'trading': 418,\n",
       " 'manufacture': 419,\n",
       " 'climate': 420,\n",
       " 'drive': 421,\n",
       " 'nutritionist': 422,\n",
       " 'tart': 423,\n",
       " 'funnel': 424,\n",
       " 'small': 425,\n",
       " 'oat': 426,\n",
       " 'difficulty': 427,\n",
       " 'instrument': 428,\n",
       " 'ancient': 429,\n",
       " 'cure': 430,\n",
       " 'flavor': 431,\n",
       " 'replace': 432,\n",
       " 'western': 433,\n",
       " 'professional': 434,\n",
       " 'second': 435,\n",
       " 'candy': 436,\n",
       " 'info': 437,\n",
       " 'bomb': 438,\n",
       " 'prone': 439,\n",
       " 'disappointed': 440,\n",
       " 'dentist': 441,\n",
       " 'loads': 442,\n",
       " 'careful': 443,\n",
       " 'sell': 444,\n",
       " 'vertical': 445,\n",
       " 'wrap': 446,\n",
       " 'carry': 447,\n",
       " 'corner': 448,\n",
       " 'facts': 449,\n",
       " 'durable': 450,\n",
       " 'consume': 451,\n",
       " 'apple': 452,\n",
       " 'endless': 453,\n",
       " 'moisture': 454,\n",
       " 'different': 455,\n",
       " 'calories': 456,\n",
       " 'ruin': 457,\n",
       " 'merry': 458,\n",
       " 'birthday': 459,\n",
       " 'msg': 460,\n",
       " 'gifting': 461,\n",
       " 'certain': 462,\n",
       " 'wait': 463,\n",
       " 'fats': 464,\n",
       " 'pretty': 465,\n",
       " 'break': 466,\n",
       " 'dietary': 467,\n",
       " 'inferior': 468,\n",
       " 'crisps': 469,\n",
       " 'believe': 470,\n",
       " 'repeat': 471,\n",
       " 'dorsher': 472,\n",
       " 'antihistamines': 473,\n",
       " 'meet': 474,\n",
       " 'menthol': 475,\n",
       " 'hawaii': 476,\n",
       " 'lunch': 477,\n",
       " 'luck': 478,\n",
       " 'active': 479,\n",
       " 'border': 480,\n",
       " 'munch': 481,\n",
       " 'hint': 482,\n",
       " 'initially': 483,\n",
       " 'delightful': 484,\n",
       " 'sonoma': 485,\n",
       " 'church': 486,\n",
       " 'personally': 487,\n",
       " 'spreadable': 488,\n",
       " 'risk': 489,\n",
       " 'sauce': 490,\n",
       " 'vibrantly': 491,\n",
       " 'baby': 492,\n",
       " 'mixture': 493,\n",
       " 'blow': 494,\n",
       " 'paper': 495,\n",
       " 'warm': 496,\n",
       " 'boot': 497,\n",
       " 'sum': 498,\n",
       " 'add': 499,\n",
       " 'constantly': 500,\n",
       " 'tbs': 501,\n",
       " 'reason': 502,\n",
       " 'miss': 503,\n",
       " 'valilla': 504,\n",
       " 'yada': 505,\n",
       " 'cooking': 506,\n",
       " 'hopefully': 507,\n",
       " 'doctor': 508,\n",
       " 'tub': 509,\n",
       " 'hospital': 510,\n",
       " 'heap': 511,\n",
       " 'crunch': 512,\n",
       " 'alcoholic': 513,\n",
       " 'grab': 514,\n",
       " 'carbonate': 515,\n",
       " 'hesitate': 516,\n",
       " 'press': 517,\n",
       " 'plan': 518,\n",
       " 'robusta': 519,\n",
       " 'compensate': 520,\n",
       " 'whivh': 521,\n",
       " 'candle': 522,\n",
       " 'touch': 523,\n",
       " 'consult': 524,\n",
       " 'gone': 525,\n",
       " 'finished': 526,\n",
       " 'fault': 527,\n",
       " 'cue': 528,\n",
       " 'particular': 529,\n",
       " 'casual': 530,\n",
       " 'bitter': 531,\n",
       " 'drawing': 532,\n",
       " 'rid': 533,\n",
       " 'carrot': 534,\n",
       " 'fully': 535,\n",
       " 'retain': 536,\n",
       " 'strange': 537,\n",
       " 'outside': 538,\n",
       " 'thailand': 539,\n",
       " 'resinous': 540,\n",
       " 'admit': 541,\n",
       " 'url': 542,\n",
       " 'diabetic': 543,\n",
       " 'straight': 544,\n",
       " 'dinner': 545,\n",
       " 'gradient': 546,\n",
       " 'practice': 547,\n",
       " 'royal': 548,\n",
       " 'cheddar': 549,\n",
       " 'distinct': 550,\n",
       " 'living': 551,\n",
       " 'regime': 552,\n",
       " 'dispurse': 553,\n",
       " 'ratio': 554,\n",
       " 'comfort': 555,\n",
       " 'include': 556,\n",
       " 'clementine': 557,\n",
       " 'giant': 558,\n",
       " 'compulsive': 559,\n",
       " 'oregano': 560,\n",
       " 'super': 561,\n",
       " 'remove': 562,\n",
       " 'guy': 563,\n",
       " 'guess': 564,\n",
       " 'holiday': 565,\n",
       " 'dry': 566,\n",
       " 'aroma': 567,\n",
       " 'little': 568,\n",
       " 'suppress': 569,\n",
       " 'cause': 570,\n",
       " 'gray': 571,\n",
       " 'pillowy': 572,\n",
       " 'crumble': 573,\n",
       " 'gallon': 574,\n",
       " 'underwhelme': 575,\n",
       " 'reminiscent': 576,\n",
       " 'snob': 577,\n",
       " 'activity': 578,\n",
       " 'mouse': 579,\n",
       " 'agreeable': 580,\n",
       " 'wafer': 581,\n",
       " 'wax': 582,\n",
       " 'whatsoever': 583,\n",
       " 'convert': 584,\n",
       " 'ago': 585,\n",
       " 'curry': 586,\n",
       " 'offering': 587,\n",
       " 'relaxing': 588,\n",
       " 'deliver': 589,\n",
       " 'counter': 590,\n",
       " 'read': 591,\n",
       " 'thought': 592,\n",
       " 'neat': 593,\n",
       " 'cutter': 594,\n",
       " 'popular': 595,\n",
       " 'dusty': 596,\n",
       " 'kashmiri': 597,\n",
       " 'hunger': 598,\n",
       " 'sir': 599,\n",
       " 'blah': 600,\n",
       " 'paint': 601,\n",
       " 'hunk': 602,\n",
       " 'opinion': 603,\n",
       " 'rainy': 604,\n",
       " 'day': 605,\n",
       " 'pilon': 606,\n",
       " 'tortellini': 607,\n",
       " 'pristine': 608,\n",
       " 'solidify': 609,\n",
       " 'suspiciously': 610,\n",
       " 'fruit': 611,\n",
       " 'specific': 612,\n",
       " 'joe': 613,\n",
       " 'merely': 614,\n",
       " 'genious': 615,\n",
       " 'soooooooo': 616,\n",
       " 'mask': 617,\n",
       " 'away': 618,\n",
       " 'cover': 619,\n",
       " 'disintegrate': 620,\n",
       " 'opt': 621,\n",
       " 'fit': 622,\n",
       " 'seasonal': 623,\n",
       " 'bland': 624,\n",
       " 'inner': 625,\n",
       " 'moister': 626,\n",
       " 'crispy': 627,\n",
       " 'basketball': 628,\n",
       " 'establish': 629,\n",
       " 'robust': 630,\n",
       " 'michael': 631,\n",
       " 'craft': 632,\n",
       " 'meal': 633,\n",
       " 'increasingly': 634,\n",
       " 'takes': 635,\n",
       " 'wonderful': 636,\n",
       " 'available': 637,\n",
       " 'rood': 638,\n",
       " 'additional': 639,\n",
       " 'mucus': 640,\n",
       " 'review': 641,\n",
       " 'health': 642,\n",
       " 'coloring': 643,\n",
       " 'barbeque': 644,\n",
       " 'illness': 645,\n",
       " 'cosr': 646,\n",
       " 'dessert': 647,\n",
       " 'food': 648,\n",
       " 'scrumptious': 649,\n",
       " 'essential': 650,\n",
       " 'varied': 651,\n",
       " 'onion': 652,\n",
       " 'instantly': 653,\n",
       " 'group': 654,\n",
       " 'seat': 655,\n",
       " 'preparation': 656,\n",
       " 'budget': 657,\n",
       " 'lb': 658,\n",
       " 'accordingly': 659,\n",
       " 'apart': 660,\n",
       " 'breathe': 661,\n",
       " 'panang': 662,\n",
       " 'amazed': 663,\n",
       " 'acupuncture': 664,\n",
       " 'load': 665,\n",
       " 'overpriced': 666,\n",
       " 'customer': 667,\n",
       " 'room': 668,\n",
       " 'man': 669,\n",
       " 'compare': 670,\n",
       " 'manufacturer': 671,\n",
       " 'toasted': 672,\n",
       " 'overall': 673,\n",
       " 'brown': 674,\n",
       " 'amiss': 675,\n",
       " 'sinuse': 676,\n",
       " 'strengthen': 677,\n",
       " 'rememer': 678,\n",
       " 'start': 679,\n",
       " 'define': 680,\n",
       " 'quit': 681,\n",
       " 'wife': 682,\n",
       " 'affect': 683,\n",
       " 'worthwhile': 684,\n",
       " 'choose': 685,\n",
       " 'upscale': 686,\n",
       " 'sprinkle': 687,\n",
       " 'teaspoon': 688,\n",
       " 'shake': 689,\n",
       " 'intact': 690,\n",
       " 'shell': 691,\n",
       " 'frosting': 692,\n",
       " 'insoluble': 693,\n",
       " 'totally': 694,\n",
       " 'energizing': 695,\n",
       " 'douwe': 696,\n",
       " 'hey': 697,\n",
       " 'splotchy': 698,\n",
       " 'gently': 699,\n",
       " 'contain': 700,\n",
       " 'helpful': 701,\n",
       " 'proof': 702,\n",
       " 'complaint': 703,\n",
       " 'blacken': 704,\n",
       " 'ridiculously': 705,\n",
       " 'rosette': 706,\n",
       " 'content': 707,\n",
       " 'saltiness': 708,\n",
       " 'parmesan': 709,\n",
       " 'boil': 710,\n",
       " 'traditional': 711,\n",
       " 'precautious': 712,\n",
       " 'gobble': 713,\n",
       " 'pepper': 714,\n",
       " 'nutrient': 715,\n",
       " 'current': 716,\n",
       " 'waxy': 717,\n",
       " 'state': 718,\n",
       " 'promote': 719,\n",
       " 'vacuum': 720,\n",
       " 'favorite': 721,\n",
       " 'view': 722,\n",
       " 'sniffle': 723,\n",
       " 'blueberry': 724,\n",
       " 'degrade': 725,\n",
       " 'weird': 726,\n",
       " 'switzerland': 727,\n",
       " 'measuring': 728,\n",
       " 'effectively': 729,\n",
       " 'diabetes': 730,\n",
       " 'subscription': 731,\n",
       " 'currently': 732,\n",
       " 'yucky': 733,\n",
       " 'oriental': 734,\n",
       " 'ideal': 735,\n",
       " 'classmate': 736,\n",
       " 'plenty': 737,\n",
       " 'scorch': 738,\n",
       " 'soda': 739,\n",
       " 'dayquil': 740,\n",
       " 'round': 741,\n",
       " 'lightly': 742,\n",
       " 'rec': 743,\n",
       " 'delighted': 744,\n",
       " 'retail': 745,\n",
       " 'offer': 746,\n",
       " 'relative': 747,\n",
       " 'pound': 748,\n",
       " 'okay': 749,\n",
       " 'dds': 750,\n",
       " 'poke': 751,\n",
       " 'surface': 752,\n",
       " 'spot': 753,\n",
       " 'success': 754,\n",
       " 'chemo': 755,\n",
       " 'pricing': 756,\n",
       " 'dust': 757,\n",
       " 'tran': 758,\n",
       " 'drinker': 759,\n",
       " 'endorsement': 760,\n",
       " 'tone': 761,\n",
       " 'distract': 762,\n",
       " 'human': 763,\n",
       " 'rubber': 764,\n",
       " 'peanut': 765,\n",
       " 'inhaler': 766,\n",
       " 'milk': 767,\n",
       " 'series': 768,\n",
       " 'crap': 769,\n",
       " 'screw': 770,\n",
       " 'wat': 771,\n",
       " 'ticket': 772,\n",
       " 'solid': 773,\n",
       " 'macaroon': 774,\n",
       " 'eye': 775,\n",
       " 'downright': 776,\n",
       " 'phony': 777,\n",
       " 'funny': 778,\n",
       " 'powerbar': 779,\n",
       " 'soo': 780,\n",
       " 'nicely': 781,\n",
       " 'yuk': 782,\n",
       " 'dewy': 783,\n",
       " 'oz': 784,\n",
       " 'patty': 785,\n",
       " 'pack': 786,\n",
       " 'metallic': 787,\n",
       " 'gum': 788,\n",
       " 'glittery': 789,\n",
       " 'whip': 790,\n",
       " 'close': 791,\n",
       " 'teal': 792,\n",
       " 'dab': 793,\n",
       " 'cooler': 794,\n",
       " 'xylitol': 795,\n",
       " 'easter': 796,\n",
       " 'ashtray': 797,\n",
       " 'negative': 798,\n",
       " 'img': 799,\n",
       " 'weekend': 800,\n",
       " 'consumption': 801,\n",
       " 'lid': 802,\n",
       " 'wholesome': 803,\n",
       " 'burn': 804,\n",
       " 'process': 805,\n",
       " 'korean': 806,\n",
       " 'experienced': 807,\n",
       " 'wise': 808,\n",
       " 'type': 809,\n",
       " 'redeem': 810,\n",
       " 'wide': 811,\n",
       " 'tract': 812,\n",
       " 'stamp': 813,\n",
       " 'donuts': 814,\n",
       " 'fajita': 815,\n",
       " 'stove': 816,\n",
       " 'see': 817,\n",
       " 'various': 818,\n",
       " 'stimulate': 819,\n",
       " 'increase': 820,\n",
       " 'wad': 821,\n",
       " 'prudhomme': 822,\n",
       " 'koenig': 823,\n",
       " 'help': 824,\n",
       " 'give': 825,\n",
       " 'soothe': 826,\n",
       " 'outer': 827,\n",
       " 'friend': 828,\n",
       " 'medium': 829,\n",
       " 'store': 830,\n",
       " 'pareil': 831,\n",
       " 'bold': 832,\n",
       " 'pick': 833,\n",
       " 'send': 834,\n",
       " 'deep': 835,\n",
       " 'brushing': 836,\n",
       " 'heart': 837,\n",
       " 'notice': 838,\n",
       " 'serving': 839,\n",
       " 'water': 840,\n",
       " 'new': 841,\n",
       " 'physician': 842,\n",
       " 'baking': 843,\n",
       " 'weight': 844,\n",
       " 'completely': 845,\n",
       " 'number': 846,\n",
       " 'strawberry': 847,\n",
       " 'stock': 848,\n",
       " 'overseas': 849,\n",
       " 'cast': 850,\n",
       " 'rate': 851,\n",
       " 'replenish': 852,\n",
       " 'medicines': 853,\n",
       " 'sip': 854,\n",
       " 'kaniwa': 855,\n",
       " 'carb': 856,\n",
       " 'stupid': 857,\n",
       " 'title': 858,\n",
       " 'clumping': 859,\n",
       " 'arsnic': 860,\n",
       " 'realize': 861,\n",
       " 'description': 862,\n",
       " 'initial': 863,\n",
       " 'starch': 864,\n",
       " 'loves': 865,\n",
       " 'infact': 866,\n",
       " 'waaaaay': 867,\n",
       " 'consistent': 868,\n",
       " 'child': 869,\n",
       " 'probably': 870,\n",
       " 'cabinet': 871,\n",
       " 'aste': 872,\n",
       " 'tingly': 873,\n",
       " 'licorice': 874,\n",
       " 'maybe': 875,\n",
       " 'texture': 876,\n",
       " 'prescribed': 877,\n",
       " 'temperature': 878,\n",
       " 'locally': 879,\n",
       " 'granulate': 880,\n",
       " 'gladden': 881,\n",
       " 'genuine': 882,\n",
       " 'evidence': 883,\n",
       " 'traditionals': 884,\n",
       " 'interior': 885,\n",
       " 'thaw': 886,\n",
       " 'quickly': 887,\n",
       " 'prescription': 888,\n",
       " 'department': 889,\n",
       " 'clearly': 890,\n",
       " 'support': 891,\n",
       " 'jam': 892,\n",
       " 'sooooo': 893,\n",
       " 'crease': 894,\n",
       " 'provide': 895,\n",
       " 'refreshing': 896,\n",
       " 'busy': 897,\n",
       " 'please': 898,\n",
       " 'level': 899,\n",
       " 'rim': 900,\n",
       " 'augment': 901,\n",
       " 'scratchy': 902,\n",
       " 'dame': 903,\n",
       " 'typical': 904,\n",
       " 'cakepop': 905,\n",
       " 'automatic': 906,\n",
       " 'applesauce': 907,\n",
       " 'live': 908,\n",
       " 'tad': 909,\n",
       " 'lover': 910,\n",
       " 'tapioca': 911,\n",
       " 'force': 912,\n",
       " 'blue': 913,\n",
       " 'anithistamine': 914,\n",
       " 'apparatus': 915,\n",
       " 'velvet': 916,\n",
       " 'task': 917,\n",
       " 'glove': 918,\n",
       " 'zero': 919,\n",
       " 'bear': 920,\n",
       " 'yeah': 921,\n",
       " 'hamburger': 922,\n",
       " 'multiple': 923,\n",
       " 'biscuit': 924,\n",
       " 'sorbitan': 925,\n",
       " 'smoothy': 926,\n",
       " 'absorption': 927,\n",
       " 'one': 928,\n",
       " 'incorporate': 929,\n",
       " 'toner': 930,\n",
       " 'joann': 931,\n",
       " 'slide': 932,\n",
       " 'boxed': 933,\n",
       " 'relate': 934,\n",
       " 'signature': 935,\n",
       " 'hesitation': 936,\n",
       " 'metal': 937,\n",
       " 'wouldn': 938,\n",
       " 'garage': 939,\n",
       " 'york': 940,\n",
       " 'nose': 941,\n",
       " 'resist': 942,\n",
       " 'garbage': 943,\n",
       " 'barely': 944,\n",
       " 'produce': 945,\n",
       " 'cow': 946,\n",
       " 'hss': 947,\n",
       " 'ethnic': 948,\n",
       " 'bubbles': 949,\n",
       " 'doesn': 950,\n",
       " 'tell': 951,\n",
       " 'afternoon': 952,\n",
       " 'ad': 953,\n",
       " 'anniversary': 954,\n",
       " 'atom': 955,\n",
       " 'lucky': 956,\n",
       " 'metropolis': 957,\n",
       " 'suit': 958,\n",
       " 'patient': 959,\n",
       " 'tightly': 960,\n",
       " 'vote': 961,\n",
       " 'exclude': 962,\n",
       " 'batch': 963,\n",
       " 'herbal': 964,\n",
       " 've': 965,\n",
       " 'fortunate': 966,\n",
       " 'letter': 967,\n",
       " 'require': 968,\n",
       " 'stain': 969,\n",
       " 'expire': 970,\n",
       " 'lemongrass': 971,\n",
       " 'tin': 972,\n",
       " 'resolve': 973,\n",
       " 'release': 974,\n",
       " 'insulate': 975,\n",
       " 'significant': 976,\n",
       " 'undertone': 977,\n",
       " 'kettle': 978,\n",
       " 'mini': 979,\n",
       " 'pancake': 980,\n",
       " 'lingo': 981,\n",
       " 'simmer': 982,\n",
       " 'couldln': 983,\n",
       " 'complain': 984,\n",
       " 'milled': 985,\n",
       " 'treater': 986,\n",
       " 'buck': 987,\n",
       " 'yogurt': 988,\n",
       " 'definite': 989,\n",
       " 'wasatch': 990,\n",
       " 'minors': 991,\n",
       " 'bottle': 992,\n",
       " 'easier': 993,\n",
       " 'hesitant': 994,\n",
       " 'clear': 995,\n",
       " 'office': 996,\n",
       " 'oral': 997,\n",
       " 'medical': 998,\n",
       " 'design': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f548bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list_to_vec(word_list):\n",
    "    'return one-hot encoding of word list'\n",
    "    vec = [0] * n_corpus\n",
    "    for word in word_list:\n",
    "        vec[word_to_ix[word]] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9518f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words is available in in the sklearn open source library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer=CountVectorizer(lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6b1fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words=vectorizer.fit_transform(df_limit['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4087dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_names=vectorizer.get_feature_names()\n",
    "bow_df=pd.DataFrame(bag_of_words.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3441700f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>09</th>\n",
       "      <th>0g</th>\n",
       "      <th>0mg</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>10x</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yuk</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zero</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zing</th>\n",
       "      <th>zone</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuccini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 4093 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  09  0g  0mg  10  100  101  10x  11  ...  yucky  yuk  yum  yummy  \\\n",
       "0   0    0   0   0    0   0    0    0    0   0  ...      0    0    0      0   \n",
       "1   0    0   0   0    0   0    0    0    0   0  ...      0    0    0      0   \n",
       "2   0    0   0   0    0   0    0    0    0   0  ...      0    0    0      0   \n",
       "\n",
       "   zero  zinc  zing  zone  zucchini  zuccini  \n",
       "0     0     0     0     0         0        0  \n",
       "1     0     0     0     0         0        0  \n",
       "2     0     0     0     0         0        0  \n",
       "\n",
       "[3 rows x 4093 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f32861ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_similarity generates a 2D array representing the cosine similarity from 0-1 between each text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(bag_of_words)\n",
    "# finding the max cosine similarity value in the resulting array, there are a number of built in numpy functions to get the max value but the trick is skipping the n, n index because they will always be 1\n",
    "max_value=0\n",
    "max_x=0\n",
    "max_y=0\n",
    "for each_row in range(len(similarity)): \n",
    "    for each_column in range(len(similarity[0])): \n",
    "        if similarity[each_row][each_column]>max_value and each_row!=each_column:\n",
    "            max_value=similarity[each_row][each_column]\n",
    "            max_y=each_row\n",
    "            max_x=each_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44be4c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000004 997 996\n"
     ]
    }
   ],
   "source": [
    "print(max_value, max_x, max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c96f0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# Not sure about best way to use one-hot vector sv create sparese train and test matrix. Hmm.\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_clean['text'].tolist(), df_clean['overall'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1bbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=100.0, random_state=1, solver='lbfgs', multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb722233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model? Predictive vectors and then test?\n",
    "# https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bda16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

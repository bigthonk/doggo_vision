{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt classification algorithm to be score predictor. How to do this? \n",
    "# First step - read and load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44eee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch setup and import data\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# define nn\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# train\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "from torch.optim import Adam\n",
    "\n",
    "# evaluate performance\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7006b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879914d",
   "metadata": {},
   "source": [
    "## Read Functions\n",
    "\n",
    "raw data has been read in, cleaned and saved to csv. I assume this is not best practice? More suitable to build the cleaning into the dataloader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c06b75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse(path):\n",
    "#     g = gzip.open(path, 'rb')\n",
    "#     for l in g:\n",
    "#         yield json.loads(l)\n",
    "\n",
    "# def getDF(path):\n",
    "#     i = 0\n",
    "#     df = {}\n",
    "#     for d in parse(path):\n",
    "#         df[i] = d\n",
    "#         i += 1\n",
    "#     return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69fea7",
   "metadata": {},
   "source": [
    "## Clean and preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11715088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleaner(df):\n",
    "#     \"Extract relevant text from DataFrame using a regex\"\n",
    "#     # Regex pattern for only alphanumeric, hyphenated text with 3 or more chars\n",
    "#     pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\n",
    "#     df['clean'] = df['reviewText'].str.findall(pattern).str.join(' ')\n",
    "#     return df\n",
    "\n",
    "# def lemmatize_pipe(doc):\n",
    "#     lemma_list = [str(tok.lemma_).lower() for tok in doc\n",
    "#                   if tok.is_alpha and tok.text.lower() not in stopwords] \n",
    "#     return lemma_list\n",
    "\n",
    "# def preprocess_pipe(texts):\n",
    "#     preproc_pipe = []\n",
    "#     for doc in nlp.pipe(texts, batch_size=20):\n",
    "#         preproc_pipe.append(lemmatize_pipe(doc))\n",
    "#     return preproc_pipe\n",
    "\n",
    "# def chunker(iterable, total_length, chunksize):\n",
    "#     return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "# def flatten(list_of_lists):\n",
    "#     \"Flatten a list of lists to a combined list\"\n",
    "#     return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# def process_chunk(texts):\n",
    "#     preproc_pipe = []\n",
    "#     for doc in nlp.pipe(texts, batch_size=20):\n",
    "#         preproc_pipe.append(lemmatize_pipe(doc))\n",
    "#     return preproc_pipe\n",
    "\n",
    "# def preprocess_parallel(texts, chunksize=100):\n",
    "#     executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n",
    "#     do = delayed(process_chunk)\n",
    "#     tasks = (do(chunk) for chunk in chunker(texts, len(df_clean), chunksize=chunksize))\n",
    "#     result = executor(tasks)\n",
    "#     return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4dad84b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[adverse, comment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[gift, college, student]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[like, strong, tea, little, strong]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                 text\n",
       "0      5.0                   [adverse, comment]\n",
       "1      5.0             [gift, college, student]\n",
       "2      5.0  [like, strong, tea, little, strong]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_limit = 1143470 # 100000\n",
    "# set_chunksize = 500\n",
    "\n",
    "# stopwords = text.ENGLISH_STOP_WORDS\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# raw_df = getDF('../../../../data/Grocery_and_Gourmet_Food_5.json.gz')\n",
    "# raw_df = raw_df.dropna(subset = [\"reviewText\"])\n",
    "\n",
    "# df_limit = raw_df.head(n_limit)\n",
    "# df_clean = cleaner(df_limit)\n",
    "\n",
    "# df_clean['text'] = preprocess_parallel(df_clean['clean'], chunksize=set_chunksize)\n",
    "\n",
    "# df_clean = df_clean[[\"overall\", \"text\"]]\n",
    "\n",
    "# # save df_clean as csv\n",
    "# df_clean.to_csv('df_clean.csv', index=False)\n",
    "# df_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fed6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# df_clean=pd.read_csv(\"df_clean.csv\")\n",
    "\n",
    "vocab = build_vocab_from_iterator(list(df_clean['text']), min_freq=1, specials=[\"<UNK>\"])\n",
    "\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63317105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95765"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0b1088e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['hello',\n",
       "  'how',\n",
       "  'are',\n",
       "  'you',\n",
       "  '?',\n",
       "  ',',\n",
       "  'welcome',\n",
       "  'to',\n",
       "  'coderzcolumn',\n",
       "  '!',\n",
       "  '!'],\n",
       " [4189, 0, 41063, 11466, 0, 0, 2122, 1220, 0, 0, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Hello how are you?, Welcome to CoderzColumn!!\")\n",
    "indexes = vocab(tokens)\n",
    "\n",
    "tokens, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3123edb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"<UNK>\"] ## Coderzcolumn word is mapped to unknown as it's new and not present in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test and train dataset from df_clean &/or df_clean with list of lists... \n",
    "# from torchtext.data.functional import to_map_style_dataset - converts iterable dataset to map_style_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0accfd",
   "metadata": {},
   "source": [
    "## Questions for Dan: \n",
    "* Is it silly to perform nlp cleaning as a separate step, save to csv and then load that in 'MyDataset'? Some smarter approach?\n",
    "* I feel like it would be useful to be able to readily edit transforms. \n",
    "* \n",
    "\n",
    "* put two batches through, 1 in each format and compare\n",
    "* move heavy lifting cleaning steps into data_loader\n",
    "* could compare categories to continuous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc15940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f904a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    " \n",
    "    def __init__(self,indices):\n",
    "        self.df_clean = pd.read_csv(\"df_clean.csv\").iloc[indices]\n",
    "\n",
    "    #     x=price_df.iloc[:,0:8].values\n",
    "    #     y=price_df.iloc[:,8].values\n",
    "\n",
    "#         self.x_train = df_clean[\"text\"]\n",
    "#         y = df_clean[\"overall\"].values\n",
    "\n",
    "# #         self.x_train=torch.tensor(x,dtype=torch.int32)# Don't convert text to tensor yet \n",
    "#         self.y_train=torch.tensor(y, dtype=torch.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_clean[\"overall\"])\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.df_clean.iloc[idx, 1], self.df_clean.iloc[idx, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "557773fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, validate and test\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# labels\n",
    "dataY = df_clean[\"overall\"]\n",
    "\n",
    "# features\n",
    "dataX_dummy = range(len(df_clean[\"text\"])) \n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "index_train, index_test, y_train, y_test = train_test_split(dataX_dummy, dataY, test_size=1 - train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "index_val, index_test, y_val, y_test = train_test_split(index_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), stratify=y_test) \n",
    "\n",
    "\n",
    "train_set = MyDataset(index_train)\n",
    "val_set = MyDataset(index_val)\n",
    "test_set = MyDataset(index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2042f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"['smooth', 'orange', 'juice', 'doesn', 'bitter', 'taste', 'process', 'do', 'buy']\", 5.0)\n",
      "(\"['great', 'travel', 'nausea']\", 5.0)\n",
      "(\"['try', 'keep', 'heavy', 'cream', 'freeze', 'small', 'need', 'freeze', 'destroy', 'taste', 'texture', 'opinion', 'detest', 'buy', 'half', 'pint', 'use', 'half', 'ultimately', 'throw', 'rest', 'heavy', 'cream', 'powder', 'add', 'just', 'tablespoon', 'creamy', 'soup', 'sauce', 'just', 'extra', 'bit', 'creaminess', 'won', 'beat', 'fresh', 'cream', 'need', 'bit', 'cream', 'recipe']\", 5.0)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])\n",
    "print(val_set[0])\n",
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab33ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will/(may?) look different because text is already a list of words... Still want to convert to tokens. \n",
    "# Should I normalize Y/scores data prior? I don't think that would matter?\n",
    "\n",
    "max_words = 25\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    X = [vocab(tokenizer(text)) for text in X]\n",
    "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
    "\n",
    "    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y, dtype=torch.int32)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset , batch_size=1024, collate_fn=vectorize_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "748cbdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/datapipes/iter/combining.py:249: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 25]) torch.Size([1024])\n",
      "---\n",
      "tensor([3, 3, 3,  ..., 1, 2, 3], dtype=torch.int32)\n",
      "---\n",
      "tensor([[    0,  3396, 27527,  ..., 11367, 27527,    56],\n",
      "        [ 1868, 62375,   400,  ..., 72484,     0,  3319],\n",
      "        [    0,  1220,  8240,  ..., 10388,     0,  6711],\n",
      "        ...,\n",
      "        [ 1687,     0,   743,  ..., 23459,  1149,  6146],\n",
      "        [ 1209,     0,     0,  ...,     0, 21961,     0],\n",
      "        [ 3069,  8019,     0,  ..., 16088,  7524, 68073]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(X.shape, Y.shape)\n",
    "    print(\"---\")\n",
    "    print(Y)\n",
    "    print(\"---\")\n",
    "    print(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "76cb2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change classifier to be score predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 50\n",
    "hidden_dim = 50\n",
    "n_layers=1\n",
    "\n",
    "right now the final layer is len(target_classes...) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57f6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 50\n",
    "hidden_dim = 50\n",
    "n_layers=1\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
    "        self.rnn = nn.RNN(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, len(target_classes))\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.embedding_layer(X_batch)\n",
    "        output, hidden = self.rnn(embeddings, torch.randn(n_layers, len(X_batch), hidden_dim))\n",
    "        return self.linear(output[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f238af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
